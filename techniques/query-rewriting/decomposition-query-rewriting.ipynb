{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain langchain-openai python-dotenv rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "schema = {\n",
    "    \"title\": \"DecompositionQueryRewriting\",\n",
    "    \"description\": \"Generate multiple sub-query variations\",\n",
    "    \"type\": \"object\", \n",
    "    \"properties\": {\n",
    "        \"queries\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"},\n",
    "            \"description\": \"List of sub-queries\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\").with_structured_output(schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\nYou are a helpful assistant that generates multiple sub-questions related to an input question. \\n\\nThe goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. Return the response in JSON format with up to 5 sub-queries. \\n\\nGenerate multiple search queries related to: what is langchain?\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "multi_query_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. Return the response in JSON format with up to 5 sub-queries. \\n\n",
    "Generate multiple search queries related to: {question}\n",
    "\"\"\"\n",
    ")\n",
    "multi_query_prompt.format(question=\"what is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'queries'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'What are the best practices for rewriting a complex query into simpler sub-queries?'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'How can one efficiently retrieve documents in parallel using Langchain?'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'What methods can be used to break down a query into multiple sub-questions for better processing?'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'What tools or libraries does Langchain provide for handling parallel document retrieval?'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'How does parallel document retrieval improve the performance of query processing in Langchain?'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'queries'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'What are the best practices for rewriting a complex query into simpler sub-queries?'\u001b[0m,\n",
       "        \u001b[32m'How can one efficiently retrieve documents in parallel using Langchain?'\u001b[0m,\n",
       "        \u001b[32m'What methods can be used to break down a query into multiple sub-questions for better processing?'\u001b[0m,\n",
       "        \u001b[32m'What tools or libraries does Langchain provide for handling parallel document retrieval?'\u001b[0m,\n",
       "        \u001b[32m'How does parallel document retrieval improve the performance of query processing in Langchain?'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "llm_chain = multi_query_prompt | chat_model\n",
    "result = llm_chain.invoke({\"question\": \"how to rewrite a query into multiple sub-queries and retrieve relevant documents in parallel in langchain?\"})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
