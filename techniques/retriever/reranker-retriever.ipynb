{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain flashrank langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'relevance_score'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float32</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9995491</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'/Users/loki/Documents/work/Personal/learnings/rag-cookbook-langchain/techniques/retriever/aiaun.pdf'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creator'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'LaTeX with hyperref'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'moddate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-04-10T21:11:43+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'subject'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'trapped'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/False'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'producer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pdfTeX-1.40.25'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_pages'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creationdate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-04-10T21:11:43+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'ptex.fullbanner'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">6.3.5'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Numerous\\nefforts have since continued to push the boundaries of recurrent language models and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">encoder-decoder\\narchitectures [38, 24, 15]. Recurrent models typically factor computation along the symbol </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sequence of hidden\\nstates ht, as a function of the previous hidden state htâˆ’1 and the input for position t. This </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conditional\\ncomputation [32], while also improving model performance in case of the latter. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fundamental\\nconstraint of sequential computation, however, remains. Attention mechanisms have become an integral </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">such attention mechanisms\\nare used in conjunction with a recurrent network. In this work we propose the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">draw global dependencies between input and output. The Transformer allows for significantly more parallelization </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">eight P100 GPUs. 2 Background\\nThe goal of reducing sequential computation also forms the foundation of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">building\\nblock, computing hidden representations in parallel for all input and output positions. In these </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2. Self-attention, sometimes called </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\nlanguage modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model relying\\nentirely on self-attention to compute representations of its input and output without using </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">as additional input when generating the next. 2'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'relevance_score'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float32</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9995393</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'/Users/loki/Documents/work/Personal/learnings/rag-cookbook-langchain/techniques/retriever/aiaun.pdf'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creator'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'LaTeX with hyperref'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'moddate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-04-10T21:11:43+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'subject'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'trapped'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/False'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'producer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pdfTeX-1.40.25'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'6'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_pages'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creationdate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-04-10T21:11:43+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'ptex.fullbanner'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">6.3.5'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lengths longer than the ones encountered\\nduring training. 4 Why Self-Attention\\nIn this section we compare various</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">zn), with xi, zi âˆˆ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">our use of self-attention we\\nconsider three desiderata. One is the total computational complexity per layer. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">operations required. The third is the path length between long-range dependencies in the network. Learning </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">two input and output positions in networks composed of the\\ndifferent layer types. As noted in Table 1, a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are faster than recurrent layers when the sequence\\n6'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'relevance_score'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.float32</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.99946886</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'/Users/loki/Documents/work/Personal/learnings/rag-cookbook-langchain/techniques/retriever/aiaun.pdf'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creator'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'LaTeX with hyperref'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'moddate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-04-10T21:11:43+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'subject'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'trapped'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/False'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'producer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pdfTeX-1.40.25'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'6'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_pages'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creationdate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-04-10T21:11:43+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'ptex.fullbanner'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">6.3.5'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention. Layer Type Complexity</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 Â· d) O(1) O(1)\\nRecurrent O(n Â· d2) O(n) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">O(n)\\nConvolutional O(k Â· n Â· d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r Â· n Â· d) O(1) O(n/r)\\n3.5 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of the\\norder of the sequence, we must inject some information about the relative or absolute position of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9]. In this work, we use</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2Ï€ to 10000 Â· 2Ï€.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos. We also </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">identical results (see Table 3 row (E)).'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'id'\u001b[0m: \u001b[1;36m4\u001b[0m,\n",
       "            \u001b[32m'relevance_score'\u001b[0m: \u001b[1;35mnp.float32\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.9995491\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[32m'page'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'author'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'source'\u001b[0m: \n",
       "\u001b[32m'/Users/loki/Documents/work/Personal/learnings/rag-cookbook-langchain/techniques/retriever/aiaun.pdf'\u001b[0m,\n",
       "            \u001b[32m'creator'\u001b[0m: \u001b[32m'LaTeX with hyperref'\u001b[0m,\n",
       "            \u001b[32m'moddate'\u001b[0m: \u001b[32m'2024-04-10T21:11:43+00:00'\u001b[0m,\n",
       "            \u001b[32m'subject'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'trapped'\u001b[0m: \u001b[32m'/False'\u001b[0m,\n",
       "            \u001b[32m'keywords'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'producer'\u001b[0m: \u001b[32m'pdfTeX-1.40.25'\u001b[0m,\n",
       "            \u001b[32m'page_label'\u001b[0m: \u001b[32m'2'\u001b[0m,\n",
       "            \u001b[32m'total_pages'\u001b[0m: \u001b[1;36m15\u001b[0m,\n",
       "            \u001b[32m'creationdate'\u001b[0m: \u001b[32m'2024-04-10T21:11:43+00:00'\u001b[0m,\n",
       "            \u001b[32m'ptex.fullbanner'\u001b[0m: \u001b[32m'This is pdfTeX, Version 3.141592653-2.6-1.40.25 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTeX Live 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m kpathsea version \u001b[0m\n",
       "\u001b[32m6.3.5'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'1 Introduction\\nRecurrent neural networks, long short-term memory \u001b[0m\u001b[32m[\u001b[0m\u001b[32m13\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and gated recurrent \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32m7\u001b[0m\u001b[32m]\u001b[0m\u001b[32m neural networks\\nin particular, have been firmly established as state of the art approaches in sequence \u001b[0m\n",
       "\u001b[32mmodeling and\\ntransduction problems such as language modeling and machine translation \u001b[0m\u001b[32m[\u001b[0m\u001b[32m 35, 2, 5\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. \u001b[0m\n",
       "\u001b[32mNumerous\\nefforts have since continued to push the boundaries of recurrent language models and \u001b[0m\n",
       "\u001b[32mencoder-decoder\\narchitectures \u001b[0m\u001b[32m[\u001b[0m\u001b[32m38, 24, 15\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. Recurrent models typically factor computation along the symbol \u001b[0m\n",
       "\u001b[32mpositions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a \u001b[0m\n",
       "\u001b[32msequence of hidden\\nstates ht, as a function of the previous hidden state htâˆ’1 and the input for position t. This \u001b[0m\n",
       "\u001b[32minherently\\nsequential nature precludes parallelization within training examples, which becomes critical at \u001b[0m\n",
       "\u001b[32mlonger\\nsequence lengths, as memory constraints limit batching across examples. Recent work has \u001b[0m\n",
       "\u001b[32machieved\\nsignificant improvements in computational efficiency through factorization tricks \u001b[0m\u001b[32m[\u001b[0m\u001b[32m21\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and \u001b[0m\n",
       "\u001b[32mconditional\\ncomputation \u001b[0m\u001b[32m[\u001b[0m\u001b[32m32\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, while also improving model performance in case of the latter. The \u001b[0m\n",
       "\u001b[32mfundamental\\nconstraint of sequential computation, however, remains. Attention mechanisms have become an integral \u001b[0m\n",
       "\u001b[32mpart of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies\u001b[0m\n",
       "\u001b[32mwithout regard to their distance in\\nthe input or output sequences \u001b[0m\u001b[32m[\u001b[0m\u001b[32m2, 19\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. In all but a few cases \u001b[0m\u001b[32m[\u001b[0m\u001b[32m27\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, however, \u001b[0m\n",
       "\u001b[32msuch attention mechanisms\\nare used in conjunction with a recurrent network. In this work we propose the \u001b[0m\n",
       "\u001b[32mTransformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to \u001b[0m\n",
       "\u001b[32mdraw global dependencies between input and output. The Transformer allows for significantly more parallelization \u001b[0m\n",
       "\u001b[32mand can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on \u001b[0m\n",
       "\u001b[32meight P100 GPUs. 2 Background\\nThe goal of reducing sequential computation also forms the foundation of the \u001b[0m\n",
       "\u001b[32mExtended Neural GPU\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m16\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, ByteNet \u001b[0m\u001b[32m[\u001b[0m\u001b[32m18\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and ConvS2S \u001b[0m\u001b[32m[\u001b[0m\u001b[32m9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, all of which use convolutional neural networks as basic \u001b[0m\n",
       "\u001b[32mbuilding\\nblock, computing hidden representations in parallel for all input and output positions. In these \u001b[0m\n",
       "\u001b[32mmodels,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin\u001b[0m\n",
       "\u001b[32mthe distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult\u001b[0m\n",
       "\u001b[32mto learn dependencies between distant positions \u001b[0m\u001b[32m[\u001b[0m\u001b[32m 12\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. In the Transformer this is\\nreduced to a constant number of \u001b[0m\n",
       "\u001b[32moperations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an \u001b[0m\n",
       "\u001b[32meffect we counteract with Multi-Head Attention as\\ndescribed in section 3.2. Self-attention, sometimes called \u001b[0m\n",
       "\u001b[32mintra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a \u001b[0m\n",
       "\u001b[32mrepresentation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading \u001b[0m\n",
       "\u001b[32mcomprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence \u001b[0m\n",
       "\u001b[32mrepresentations \u001b[0m\u001b[32m[\u001b[0m\u001b[32m4, 27, 28, 22\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. End-to-end memory networks are based on a recurrent attention mechanism instead of\u001b[0m\n",
       "\u001b[32msequence-\\naligned recurrence and have been shown to perform well on simple-language question answering \u001b[0m\n",
       "\u001b[32mand\\nlanguage modeling tasks \u001b[0m\u001b[32m[\u001b[0m\u001b[32m34\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. To the best of our knowledge, however, the Transformer is the first transduction\u001b[0m\n",
       "\u001b[32mmodel relying\\nentirely on self-attention to compute representations of its input and output without using \u001b[0m\n",
       "\u001b[32msequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, \u001b[0m\n",
       "\u001b[32mmotivate\\nself-attention and discuss its advantages over models such as \u001b[0m\u001b[32m[\u001b[0m\u001b[32m17, 18\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and \u001b[0m\u001b[32m[\u001b[0m\u001b[32m9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. 3 Model \u001b[0m\n",
       "\u001b[32mArchitecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure \u001b[0m\u001b[32m[\u001b[0m\u001b[32m5, 2, 35\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. \u001b[0m\n",
       "\u001b[32mHere, the encoder maps an input sequence of symbol representations \u001b[0m\u001b[32m(\u001b[0m\u001b[32mx1, ..., xn\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to a sequence\\nof continuous \u001b[0m\n",
       "\u001b[32mrepresentations z = \u001b[0m\u001b[32m(\u001b[0m\u001b[32mz1, ..., zn\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Given z, the decoder then generates an output\\nsequence \u001b[0m\u001b[32m(\u001b[0m\u001b[32my1, ..., ym\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of symbols\u001b[0m\n",
       "\u001b[32mone element at a time. At each step the model is auto-regressive\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m10\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, consuming the previously generated symbols \u001b[0m\n",
       "\u001b[32mas additional input when generating the next. 2'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'id'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "            \u001b[32m'relevance_score'\u001b[0m: \u001b[1;35mnp.float32\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.9995393\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[32m'page'\u001b[0m: \u001b[1;36m5\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'author'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'source'\u001b[0m: \n",
       "\u001b[32m'/Users/loki/Documents/work/Personal/learnings/rag-cookbook-langchain/techniques/retriever/aiaun.pdf'\u001b[0m,\n",
       "            \u001b[32m'creator'\u001b[0m: \u001b[32m'LaTeX with hyperref'\u001b[0m,\n",
       "            \u001b[32m'moddate'\u001b[0m: \u001b[32m'2024-04-10T21:11:43+00:00'\u001b[0m,\n",
       "            \u001b[32m'subject'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'trapped'\u001b[0m: \u001b[32m'/False'\u001b[0m,\n",
       "            \u001b[32m'keywords'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'producer'\u001b[0m: \u001b[32m'pdfTeX-1.40.25'\u001b[0m,\n",
       "            \u001b[32m'page_label'\u001b[0m: \u001b[32m'6'\u001b[0m,\n",
       "            \u001b[32m'total_pages'\u001b[0m: \u001b[1;36m15\u001b[0m,\n",
       "            \u001b[32m'creationdate'\u001b[0m: \u001b[32m'2024-04-10T21:11:43+00:00'\u001b[0m,\n",
       "            \u001b[32m'ptex.fullbanner'\u001b[0m: \u001b[32m'This is pdfTeX, Version 3.141592653-2.6-1.40.25 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTeX Live 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m kpathsea version \u001b[0m\n",
       "\u001b[32m6.3.5'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence \u001b[0m\n",
       "\u001b[32mlengths longer than the ones encountered\\nduring training. 4 Why Self-Attention\\nIn this section we compare various\u001b[0m\n",
       "\u001b[32maspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one \u001b[0m\n",
       "\u001b[32mvariable-length sequence of symbol representations\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx1, ..., xn\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to another sequence of equal length \u001b[0m\u001b[32m(\u001b[0m\u001b[32mz1, ..., \u001b[0m\n",
       "\u001b[32mzn\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, with xi, zi âˆˆ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating \u001b[0m\n",
       "\u001b[32mour use of self-attention we\\nconsider three desiderata. One is the total computational complexity per layer. \u001b[0m\n",
       "\u001b[32mAnother is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential \u001b[0m\n",
       "\u001b[32moperations required. The third is the path length between long-range dependencies in the network. Learning \u001b[0m\n",
       "\u001b[32mlong-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting \u001b[0m\n",
       "\u001b[32mthe\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse \u001b[0m\n",
       "\u001b[32min the network. The shorter these paths between any combination of positions in the input\\nand output sequences, \u001b[0m\n",
       "\u001b[32mthe easier it is to learn long-range dependencies \u001b[0m\u001b[32m[\u001b[0m\u001b[32m12\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. Hence we also compare\\nthe maximum path length between any \u001b[0m\n",
       "\u001b[32mtwo input and output positions in networks composed of the\\ndifferent layer types. As noted in Table 1, a \u001b[0m\n",
       "\u001b[32mself-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a \u001b[0m\n",
       "\u001b[32mrecurrent layer requires O\u001b[0m\u001b[32m(\u001b[0m\u001b[32mn\u001b[0m\u001b[32m)\u001b[0m\u001b[32m sequential operations. In terms of\\ncomputational complexity, self-attention layers \u001b[0m\n",
       "\u001b[32mare faster than recurrent layers when the sequence\\n6'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'id'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "            \u001b[32m'relevance_score'\u001b[0m: \u001b[1;35mnp.float32\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.99946886\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[32m'page'\u001b[0m: \u001b[1;36m5\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'author'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'source'\u001b[0m: \n",
       "\u001b[32m'/Users/loki/Documents/work/Personal/learnings/rag-cookbook-langchain/techniques/retriever/aiaun.pdf'\u001b[0m,\n",
       "            \u001b[32m'creator'\u001b[0m: \u001b[32m'LaTeX with hyperref'\u001b[0m,\n",
       "            \u001b[32m'moddate'\u001b[0m: \u001b[32m'2024-04-10T21:11:43+00:00'\u001b[0m,\n",
       "            \u001b[32m'subject'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'trapped'\u001b[0m: \u001b[32m'/False'\u001b[0m,\n",
       "            \u001b[32m'keywords'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'producer'\u001b[0m: \u001b[32m'pdfTeX-1.40.25'\u001b[0m,\n",
       "            \u001b[32m'page_label'\u001b[0m: \u001b[32m'6'\u001b[0m,\n",
       "            \u001b[32m'total_pages'\u001b[0m: \u001b[1;36m15\u001b[0m,\n",
       "            \u001b[32m'creationdate'\u001b[0m: \u001b[32m'2024-04-10T21:11:43+00:00'\u001b[0m,\n",
       "            \u001b[32m'ptex.fullbanner'\u001b[0m: \u001b[32m'This is pdfTeX, Version 3.141592653-2.6-1.40.25 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTeX Live 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m kpathsea version \u001b[0m\n",
       "\u001b[32m6.3.5'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential \u001b[0m\n",
       "\u001b[32moperations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the \u001b[0m\n",
       "\u001b[32mkernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention. Layer Type Complexity\u001b[0m\n",
       "\u001b[32mper Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O\u001b[0m\u001b[32m(\u001b[0m\u001b[32mn2 Â· d\u001b[0m\u001b[32m)\u001b[0m\u001b[32m O\u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m O\u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nRecurrent O\u001b[0m\u001b[32m(\u001b[0m\u001b[32mn Â· d2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m O\u001b[0m\u001b[32m(\u001b[0m\u001b[32mn\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mO\u001b[0m\u001b[32m(\u001b[0m\u001b[32mn\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nConvolutional O\u001b[0m\u001b[32m(\u001b[0m\u001b[32mk Â· n Â· d2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m O\u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m O\u001b[0m\u001b[32m(\u001b[0m\u001b[32mlogk\u001b[0m\u001b[32m(\u001b[0m\u001b[32mn\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nSelf-Attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mrestricted\u001b[0m\u001b[32m)\u001b[0m\u001b[32m O\u001b[0m\u001b[32m(\u001b[0m\u001b[32mr Â· n Â· d\u001b[0m\u001b[32m)\u001b[0m\u001b[32m O\u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m O\u001b[0m\u001b[32m(\u001b[0m\u001b[32mn/r\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n3.5 \u001b[0m\n",
       "\u001b[32mPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use \u001b[0m\n",
       "\u001b[32mof the\\norder of the sequence, we must inject some information about the relative or absolute position of \u001b[0m\n",
       "\u001b[32mthe\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of \u001b[0m\n",
       "\u001b[32mthe encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that\u001b[0m\n",
       "\u001b[32mthe two can be summed. There are many choices of positional encodings,\\nlearned and fixed \u001b[0m\u001b[32m[\u001b[0m\u001b[32m9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. In this work, we use\u001b[0m\n",
       "\u001b[32msine and cosine functions of different frequencies:\\nP E\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpos,2i\u001b[0m\u001b[32m)\u001b[0m\u001b[32m = sin\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpos/100002i/dmodel \u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nP E\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpos,2i+1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m = \u001b[0m\n",
       "\u001b[32mcos\u001b[0m\u001b[32m(\u001b[0m\u001b[32mpos/100002i/dmodel \u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nwhere pos is the position and i is the dimension. That is, each dimension of the \u001b[0m\n",
       "\u001b[32mpositional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2Ï€ to 10000 Â· 2Ï€.\u001b[0m\n",
       "\u001b[32mWe\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative \u001b[0m\n",
       "\u001b[32mpositions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos. We also \u001b[0m\n",
       "\u001b[32mexperimented with using learned positional embeddings \u001b[0m\u001b[32m[\u001b[0m\u001b[32m9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m instead, and found that the two\\nversions produced nearly\u001b[0m\n",
       "\u001b[32midentical results \u001b[0m\u001b[32m(\u001b[0m\u001b[32msee Table 3 row \u001b[0m\u001b[32m(\u001b[0m\u001b[32mE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.document_compressors import FlashrankRerank\n",
    "from index_pipeline import vector_store\n",
    "from rich import print\n",
    "\n",
    "reranker = FlashrankRerank(top_n=3)\n",
    "vector_retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "retriever = ContextualCompressionRetriever(base_compressor=reranker, base_retriever=vector_retriever)\n",
    "retriever_result = retriever.invoke(\"why does self-attention have an advantage over recurrent layers in terms of parallelization and path length for long-range dependencies?\")\n",
    "\n",
    "print(retriever_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Self-attention has an advantage over recurrent layers in terms of parallelization and path length for long-range \n",
       "dependencies due to its architectural design. Specifically, self-attention allows for all positions in a sequence \n",
       "to be connected with a constant number of sequentially executed operations, which means that it can process inputs \n",
       "in parallel. In contrast, recurrent layers require a linear number of sequential operations <span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"font-weight: bold\">(</span>n<span style=\"font-weight: bold\">))</span>, which limits \n",
       "their ability to parallelize computations, especially as sequence lengths increase.\n",
       "\n",
       "Additionally, the path length for learning long-range dependencies is significantly shorter in self-attention \n",
       "mechanisms. In self-attention, the maximum path length between any two input and output positions is constant \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">))</span>, making it easier to learn dependencies regardless of their distance in the sequence. On the other hand, \n",
       "recurrent layers have a maximum path length that grows linearly with the sequence length <span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"font-weight: bold\">(</span>n<span style=\"font-weight: bold\">))</span>, which complicates \n",
       "the learning of long-range dependencies.\n",
       "\n",
       "In summary, self-attention's ability to connect all positions with constant operations and its constant path length\n",
       "for dependencies provide significant advantages over recurrent layers, particularly in terms of computational \n",
       "efficiency and the ability to learn long-range relationships in data.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Self-attention has an advantage over recurrent layers in terms of parallelization and path length for long-range \n",
       "dependencies due to its architectural design. Specifically, self-attention allows for all positions in a sequence \n",
       "to be connected with a constant number of sequentially executed operations, which means that it can process inputs \n",
       "in parallel. In contrast, recurrent layers require a linear number of sequential operations \u001b[1m(\u001b[0m\u001b[1;35mO\u001b[0m\u001b[1m(\u001b[0mn\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m, which limits \n",
       "their ability to parallelize computations, especially as sequence lengths increase.\n",
       "\n",
       "Additionally, the path length for learning long-range dependencies is significantly shorter in self-attention \n",
       "mechanisms. In self-attention, the maximum path length between any two input and output positions is constant \n",
       "\u001b[1m(\u001b[0m\u001b[1;35mO\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m, making it easier to learn dependencies regardless of their distance in the sequence. On the other hand, \n",
       "recurrent layers have a maximum path length that grows linearly with the sequence length \u001b[1m(\u001b[0m\u001b[1;35mO\u001b[0m\u001b[1m(\u001b[0mn\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m, which complicates \n",
       "the learning of long-range dependencies.\n",
       "\n",
       "In summary, self-attention's ability to connect all positions with constant operations and its constant path length\n",
       "for dependencies provide significant advantages over recurrent layers, particularly in terms of computational \n",
       "efficiency and the ability to learn long-range relationships in data.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from rich import print\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "response = chat_model.invoke(f\"You are an expert in the field of AI and you are given a question and a document. You need to answer the question based on the document. Here is the question: Why does self-attention have an advantage over recurrent layers in terms of parallelization and path length for long-range dependencies? Here is the document: {retriever_result}\")\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
