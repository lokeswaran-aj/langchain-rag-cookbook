{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from index_pipeline import chunks\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'producer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pdfTeX-1.40.25'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creator'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'LaTeX with hyperref'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creationdate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-04-10T21:11:43+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'moddate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-04-10T21:11:43+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'ptex.fullbanner'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">6.3.5'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'subject'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'trapped'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/False'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'/Users/loki/Documents/work/Personal/learnings/rag-cookbook-langchain/techniques/retriever/aiaun.pdf'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_pages'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'6'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lengths longer than the ones encountered\\nduring training. 4 Why Self-Attention\\nIn this section we compare various</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">zn), with xi, zi âˆˆ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">our use of self-attention we\\nconsider three desiderata. One is the total computational complexity per layer. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">operations required. The third is the path length between long-range dependencies in the network. Learning </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">two input and output positions in networks composed of the\\ndifferent layer types. As noted in Table 1, a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are faster than recurrent layers when the sequence\\n6'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'producer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pdfTeX-1.40.25'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creator'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'LaTeX with hyperref'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creationdate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-04-10T21:11:43+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'moddate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-04-10T21:11:43+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'ptex.fullbanner'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">6.3.5'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'subject'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'trapped'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/False'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'/Users/loki/Documents/work/Personal/learnings/rag-cookbook-langchain/techniques/retriever/aiaun.pdf'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_pages'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Numerous\\nefforts have since continued to push the boundaries of recurrent language models and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">encoder-decoder\\narchitectures [38, 24, 15]. Recurrent models typically factor computation along the symbol </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sequence of hidden\\nstates ht, as a function of the previous hidden state htâˆ’1 and the input for position t. This </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conditional\\ncomputation [32], while also improving model performance in case of the latter. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fundamental\\nconstraint of sequential computation, however, remains. Attention mechanisms have become an integral </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">such attention mechanisms\\nare used in conjunction with a recurrent network. In this work we propose the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">draw global dependencies between input and output. The Transformer allows for significantly more parallelization </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">eight P100 GPUs. 2 Background\\nThe goal of reducing sequential computation also forms the foundation of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">building\\nblock, computing hidden representations in parallel for all input and output positions. In these </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2. Self-attention, sometimes called </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\nlanguage modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model relying\\nentirely on self-attention to compute representations of its input and output without using </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">as additional input when generating the next. 2'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'producer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pdfTeX-1.40.25'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creator'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'LaTeX with hyperref'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creationdate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-04-10T21:11:43+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'moddate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-04-10T21:11:43+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'ptex.fullbanner'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">6.3.5'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'subject'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'trapped'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/False'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'/Users/loki/Documents/work/Personal/learnings/rag-cookbook-langchain/techniques/retriever/aiaun.pdf'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_pages'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'7'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'length n is smaller than the representation dimensionality d, which is most often the case </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">around the respective output position. This would increase the maximum\\npath length to O(n/r).'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'producer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pdfTeX-1.40.25'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creator'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'LaTeX with hyperref'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creationdate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-04-10T21:11:43+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'moddate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-04-10T21:11:43+00:00'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'ptex.fullbanner'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">6.3.5'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'subject'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'trapped'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/False'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'/Users/loki/Documents/work/Personal/learnings/rag-cookbook-langchain/techniques/retriever/aiaun.pdf'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_pages'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'7'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'We plan to investigate this approach further in future work. A single convolutional layer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with kernel width k &lt; ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the complexity\\nconsiderably, to O(k Â· n Â· d + n Â· d2). Even with k = n, however, the complexity of a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">layer,\\nthe approach we take in our model. As side benefit, self-attention could yield more interpretable models.'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'producer'\u001b[0m: \u001b[32m'pdfTeX-1.40.25'\u001b[0m,\n",
       "            \u001b[32m'creator'\u001b[0m: \u001b[32m'LaTeX with hyperref'\u001b[0m,\n",
       "            \u001b[32m'creationdate'\u001b[0m: \u001b[32m'2024-04-10T21:11:43+00:00'\u001b[0m,\n",
       "            \u001b[32m'author'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'keywords'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'moddate'\u001b[0m: \u001b[32m'2024-04-10T21:11:43+00:00'\u001b[0m,\n",
       "            \u001b[32m'ptex.fullbanner'\u001b[0m: \u001b[32m'This is pdfTeX, Version 3.141592653-2.6-1.40.25 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTeX Live 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m kpathsea version \u001b[0m\n",
       "\u001b[32m6.3.5'\u001b[0m,\n",
       "            \u001b[32m'subject'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'trapped'\u001b[0m: \u001b[32m'/False'\u001b[0m,\n",
       "            \u001b[32m'source'\u001b[0m: \n",
       "\u001b[32m'/Users/loki/Documents/work/Personal/learnings/rag-cookbook-langchain/techniques/retriever/aiaun.pdf'\u001b[0m,\n",
       "            \u001b[32m'total_pages'\u001b[0m: \u001b[1;36m15\u001b[0m,\n",
       "            \u001b[32m'page'\u001b[0m: \u001b[1;36m5\u001b[0m,\n",
       "            \u001b[32m'page_label'\u001b[0m: \u001b[32m'6'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence \u001b[0m\n",
       "\u001b[32mlengths longer than the ones encountered\\nduring training. 4 Why Self-Attention\\nIn this section we compare various\u001b[0m\n",
       "\u001b[32maspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one \u001b[0m\n",
       "\u001b[32mvariable-length sequence of symbol representations\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx1, ..., xn\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to another sequence of equal length \u001b[0m\u001b[32m(\u001b[0m\u001b[32mz1, ..., \u001b[0m\n",
       "\u001b[32mzn\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, with xi, zi âˆˆ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating \u001b[0m\n",
       "\u001b[32mour use of self-attention we\\nconsider three desiderata. One is the total computational complexity per layer. \u001b[0m\n",
       "\u001b[32mAnother is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential \u001b[0m\n",
       "\u001b[32moperations required. The third is the path length between long-range dependencies in the network. Learning \u001b[0m\n",
       "\u001b[32mlong-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting \u001b[0m\n",
       "\u001b[32mthe\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse \u001b[0m\n",
       "\u001b[32min the network. The shorter these paths between any combination of positions in the input\\nand output sequences, \u001b[0m\n",
       "\u001b[32mthe easier it is to learn long-range dependencies \u001b[0m\u001b[32m[\u001b[0m\u001b[32m12\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. Hence we also compare\\nthe maximum path length between any \u001b[0m\n",
       "\u001b[32mtwo input and output positions in networks composed of the\\ndifferent layer types. As noted in Table 1, a \u001b[0m\n",
       "\u001b[32mself-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a \u001b[0m\n",
       "\u001b[32mrecurrent layer requires O\u001b[0m\u001b[32m(\u001b[0m\u001b[32mn\u001b[0m\u001b[32m)\u001b[0m\u001b[32m sequential operations. In terms of\\ncomputational complexity, self-attention layers \u001b[0m\n",
       "\u001b[32mare faster than recurrent layers when the sequence\\n6'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'producer'\u001b[0m: \u001b[32m'pdfTeX-1.40.25'\u001b[0m,\n",
       "            \u001b[32m'creator'\u001b[0m: \u001b[32m'LaTeX with hyperref'\u001b[0m,\n",
       "            \u001b[32m'creationdate'\u001b[0m: \u001b[32m'2024-04-10T21:11:43+00:00'\u001b[0m,\n",
       "            \u001b[32m'author'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'keywords'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'moddate'\u001b[0m: \u001b[32m'2024-04-10T21:11:43+00:00'\u001b[0m,\n",
       "            \u001b[32m'ptex.fullbanner'\u001b[0m: \u001b[32m'This is pdfTeX, Version 3.141592653-2.6-1.40.25 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTeX Live 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m kpathsea version \u001b[0m\n",
       "\u001b[32m6.3.5'\u001b[0m,\n",
       "            \u001b[32m'subject'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'trapped'\u001b[0m: \u001b[32m'/False'\u001b[0m,\n",
       "            \u001b[32m'source'\u001b[0m: \n",
       "\u001b[32m'/Users/loki/Documents/work/Personal/learnings/rag-cookbook-langchain/techniques/retriever/aiaun.pdf'\u001b[0m,\n",
       "            \u001b[32m'total_pages'\u001b[0m: \u001b[1;36m15\u001b[0m,\n",
       "            \u001b[32m'page'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "            \u001b[32m'page_label'\u001b[0m: \u001b[32m'2'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'1 Introduction\\nRecurrent neural networks, long short-term memory \u001b[0m\u001b[32m[\u001b[0m\u001b[32m13\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and gated recurrent \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32m7\u001b[0m\u001b[32m]\u001b[0m\u001b[32m neural networks\\nin particular, have been firmly established as state of the art approaches in sequence \u001b[0m\n",
       "\u001b[32mmodeling and\\ntransduction problems such as language modeling and machine translation \u001b[0m\u001b[32m[\u001b[0m\u001b[32m 35, 2, 5\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. \u001b[0m\n",
       "\u001b[32mNumerous\\nefforts have since continued to push the boundaries of recurrent language models and \u001b[0m\n",
       "\u001b[32mencoder-decoder\\narchitectures \u001b[0m\u001b[32m[\u001b[0m\u001b[32m38, 24, 15\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. Recurrent models typically factor computation along the symbol \u001b[0m\n",
       "\u001b[32mpositions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a \u001b[0m\n",
       "\u001b[32msequence of hidden\\nstates ht, as a function of the previous hidden state htâˆ’1 and the input for position t. This \u001b[0m\n",
       "\u001b[32minherently\\nsequential nature precludes parallelization within training examples, which becomes critical at \u001b[0m\n",
       "\u001b[32mlonger\\nsequence lengths, as memory constraints limit batching across examples. Recent work has \u001b[0m\n",
       "\u001b[32machieved\\nsignificant improvements in computational efficiency through factorization tricks \u001b[0m\u001b[32m[\u001b[0m\u001b[32m21\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and \u001b[0m\n",
       "\u001b[32mconditional\\ncomputation \u001b[0m\u001b[32m[\u001b[0m\u001b[32m32\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, while also improving model performance in case of the latter. The \u001b[0m\n",
       "\u001b[32mfundamental\\nconstraint of sequential computation, however, remains. Attention mechanisms have become an integral \u001b[0m\n",
       "\u001b[32mpart of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies\u001b[0m\n",
       "\u001b[32mwithout regard to their distance in\\nthe input or output sequences \u001b[0m\u001b[32m[\u001b[0m\u001b[32m2, 19\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. In all but a few cases \u001b[0m\u001b[32m[\u001b[0m\u001b[32m27\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, however, \u001b[0m\n",
       "\u001b[32msuch attention mechanisms\\nare used in conjunction with a recurrent network. In this work we propose the \u001b[0m\n",
       "\u001b[32mTransformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to \u001b[0m\n",
       "\u001b[32mdraw global dependencies between input and output. The Transformer allows for significantly more parallelization \u001b[0m\n",
       "\u001b[32mand can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on \u001b[0m\n",
       "\u001b[32meight P100 GPUs. 2 Background\\nThe goal of reducing sequential computation also forms the foundation of the \u001b[0m\n",
       "\u001b[32mExtended Neural GPU\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m16\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, ByteNet \u001b[0m\u001b[32m[\u001b[0m\u001b[32m18\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and ConvS2S \u001b[0m\u001b[32m[\u001b[0m\u001b[32m9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, all of which use convolutional neural networks as basic \u001b[0m\n",
       "\u001b[32mbuilding\\nblock, computing hidden representations in parallel for all input and output positions. In these \u001b[0m\n",
       "\u001b[32mmodels,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin\u001b[0m\n",
       "\u001b[32mthe distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult\u001b[0m\n",
       "\u001b[32mto learn dependencies between distant positions \u001b[0m\u001b[32m[\u001b[0m\u001b[32m 12\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. In the Transformer this is\\nreduced to a constant number of \u001b[0m\n",
       "\u001b[32moperations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an \u001b[0m\n",
       "\u001b[32meffect we counteract with Multi-Head Attention as\\ndescribed in section 3.2. Self-attention, sometimes called \u001b[0m\n",
       "\u001b[32mintra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a \u001b[0m\n",
       "\u001b[32mrepresentation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading \u001b[0m\n",
       "\u001b[32mcomprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence \u001b[0m\n",
       "\u001b[32mrepresentations \u001b[0m\u001b[32m[\u001b[0m\u001b[32m4, 27, 28, 22\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. End-to-end memory networks are based on a recurrent attention mechanism instead of\u001b[0m\n",
       "\u001b[32msequence-\\naligned recurrence and have been shown to perform well on simple-language question answering \u001b[0m\n",
       "\u001b[32mand\\nlanguage modeling tasks \u001b[0m\u001b[32m[\u001b[0m\u001b[32m34\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. To the best of our knowledge, however, the Transformer is the first transduction\u001b[0m\n",
       "\u001b[32mmodel relying\\nentirely on self-attention to compute representations of its input and output without using \u001b[0m\n",
       "\u001b[32msequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, \u001b[0m\n",
       "\u001b[32mmotivate\\nself-attention and discuss its advantages over models such as \u001b[0m\u001b[32m[\u001b[0m\u001b[32m17, 18\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and \u001b[0m\u001b[32m[\u001b[0m\u001b[32m9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. 3 Model \u001b[0m\n",
       "\u001b[32mArchitecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure \u001b[0m\u001b[32m[\u001b[0m\u001b[32m5, 2, 35\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. \u001b[0m\n",
       "\u001b[32mHere, the encoder maps an input sequence of symbol representations \u001b[0m\u001b[32m(\u001b[0m\u001b[32mx1, ..., xn\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to a sequence\\nof continuous \u001b[0m\n",
       "\u001b[32mrepresentations z = \u001b[0m\u001b[32m(\u001b[0m\u001b[32mz1, ..., zn\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Given z, the decoder then generates an output\\nsequence \u001b[0m\u001b[32m(\u001b[0m\u001b[32my1, ..., ym\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of symbols\u001b[0m\n",
       "\u001b[32mone element at a time. At each step the model is auto-regressive\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m10\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, consuming the previously generated symbols \u001b[0m\n",
       "\u001b[32mas additional input when generating the next. 2'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'producer'\u001b[0m: \u001b[32m'pdfTeX-1.40.25'\u001b[0m,\n",
       "            \u001b[32m'creator'\u001b[0m: \u001b[32m'LaTeX with hyperref'\u001b[0m,\n",
       "            \u001b[32m'creationdate'\u001b[0m: \u001b[32m'2024-04-10T21:11:43+00:00'\u001b[0m,\n",
       "            \u001b[32m'author'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'keywords'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'moddate'\u001b[0m: \u001b[32m'2024-04-10T21:11:43+00:00'\u001b[0m,\n",
       "            \u001b[32m'ptex.fullbanner'\u001b[0m: \u001b[32m'This is pdfTeX, Version 3.141592653-2.6-1.40.25 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTeX Live 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m kpathsea version \u001b[0m\n",
       "\u001b[32m6.3.5'\u001b[0m,\n",
       "            \u001b[32m'subject'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'trapped'\u001b[0m: \u001b[32m'/False'\u001b[0m,\n",
       "            \u001b[32m'source'\u001b[0m: \n",
       "\u001b[32m'/Users/loki/Documents/work/Personal/learnings/rag-cookbook-langchain/techniques/retriever/aiaun.pdf'\u001b[0m,\n",
       "            \u001b[32m'total_pages'\u001b[0m: \u001b[1;36m15\u001b[0m,\n",
       "            \u001b[32m'page'\u001b[0m: \u001b[1;36m6\u001b[0m,\n",
       "            \u001b[32m'page_label'\u001b[0m: \u001b[32m'7'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'length n is smaller than the representation dimensionality d, which is most often the case \u001b[0m\n",
       "\u001b[32mwith\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m38\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mand byte-pair \u001b[0m\u001b[32m[\u001b[0m\u001b[32m31\u001b[0m\u001b[32m]\u001b[0m\u001b[32m representations. To improve computational performance for tasks involving\\nvery long sequences, \u001b[0m\n",
       "\u001b[32mself-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered \u001b[0m\n",
       "\u001b[32maround the respective output position. This would increase the maximum\\npath length to O\u001b[0m\u001b[32m(\u001b[0m\u001b[32mn/r\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'producer'\u001b[0m: \u001b[32m'pdfTeX-1.40.25'\u001b[0m,\n",
       "            \u001b[32m'creator'\u001b[0m: \u001b[32m'LaTeX with hyperref'\u001b[0m,\n",
       "            \u001b[32m'creationdate'\u001b[0m: \u001b[32m'2024-04-10T21:11:43+00:00'\u001b[0m,\n",
       "            \u001b[32m'author'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'keywords'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'moddate'\u001b[0m: \u001b[32m'2024-04-10T21:11:43+00:00'\u001b[0m,\n",
       "            \u001b[32m'ptex.fullbanner'\u001b[0m: \u001b[32m'This is pdfTeX, Version 3.141592653-2.6-1.40.25 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTeX Live 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m kpathsea version \u001b[0m\n",
       "\u001b[32m6.3.5'\u001b[0m,\n",
       "            \u001b[32m'subject'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'title'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "            \u001b[32m'trapped'\u001b[0m: \u001b[32m'/False'\u001b[0m,\n",
       "            \u001b[32m'source'\u001b[0m: \n",
       "\u001b[32m'/Users/loki/Documents/work/Personal/learnings/rag-cookbook-langchain/techniques/retriever/aiaun.pdf'\u001b[0m,\n",
       "            \u001b[32m'total_pages'\u001b[0m: \u001b[1;36m15\u001b[0m,\n",
       "            \u001b[32m'page'\u001b[0m: \u001b[1;36m6\u001b[0m,\n",
       "            \u001b[32m'page_label'\u001b[0m: \u001b[32m'7'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'We plan to investigate this approach further in future work. A single convolutional layer \u001b[0m\n",
       "\u001b[32mwith kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of \u001b[0m\n",
       "\u001b[32mO\u001b[0m\u001b[32m(\u001b[0m\u001b[32mn/k\u001b[0m\u001b[32m)\u001b[0m\u001b[32m convolutional layers in the case of contiguous kernels,\\nor O\u001b[0m\u001b[32m(\u001b[0m\u001b[32mlogk\u001b[0m\u001b[32m(\u001b[0m\u001b[32mn\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in the case of dilated convolutions \u001b[0m\u001b[32m[\u001b[0m\n",
       "\u001b[32m18\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are\u001b[0m\n",
       "\u001b[32mgenerally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions \u001b[0m\u001b[32m[\u001b[0m\u001b[32m 6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, however, decrease \u001b[0m\n",
       "\u001b[32mthe complexity\\nconsiderably, to O\u001b[0m\u001b[32m(\u001b[0m\u001b[32mk Â· n Â· d + n Â· d2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Even with k = n, however, the complexity of a \u001b[0m\n",
       "\u001b[32mseparable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward \u001b[0m\n",
       "\u001b[32mlayer,\\nthe approach we take in our model. As side benefit, self-attention could yield more interpretable models.'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "\n",
    "result = bm25_retriever.invoke(\"Why does self-attention have an advantage over recurrent layers in terms of parallelization and path length for long-range dependencies?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Self-attention has significant advantages over recurrent layers in terms of parallelization and path length for \n",
       "long-range dependencies due to its architectural design. \n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Parallelization**: Self-attention layers allow for a constant number of sequential operations to connect all \n",
       "positions in the input sequence, which means that they can process all elements simultaneously. In contrast, \n",
       "recurrent layers require a linear number of sequential operations <span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"font-weight: bold\">(</span>n<span style=\"font-weight: bold\">))</span>, as they process the input one step at a \n",
       "time, which inherently limits their ability to parallelize computations across different time steps. This makes \n",
       "self-attention much more efficient, especially for longer sequences, as it can leverage modern hardware \n",
       "capabilities for parallel processing.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Path Length for Long-Range Dependencies**: The ability to learn long-range dependencies is crucial in sequence\n",
       "modeling tasks. Self-attention reduces the maximum path length between any two input and output positions to a \n",
       "constant number of operations, making it easier to learn these dependencies. In recurrent networks, the path length\n",
       "grows linearly with the sequence length, which complicates the learning of relationships between distant elements \n",
       "in the sequence. Shorter paths in self-attention facilitate the learning of these long-range dependencies more \n",
       "effectively.\n",
       "\n",
       "In summary, self-attention's design allows for greater parallelization and shorter path lengths, which are critical\n",
       "for efficiently handling long-range dependencies in sequence data.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Self-attention has significant advantages over recurrent layers in terms of parallelization and path length for \n",
       "long-range dependencies due to its architectural design. \n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Parallelization**: Self-attention layers allow for a constant number of sequential operations to connect all \n",
       "positions in the input sequence, which means that they can process all elements simultaneously. In contrast, \n",
       "recurrent layers require a linear number of sequential operations \u001b[1m(\u001b[0m\u001b[1;35mO\u001b[0m\u001b[1m(\u001b[0mn\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m, as they process the input one step at a \n",
       "time, which inherently limits their ability to parallelize computations across different time steps. This makes \n",
       "self-attention much more efficient, especially for longer sequences, as it can leverage modern hardware \n",
       "capabilities for parallel processing.\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. **Path Length for Long-Range Dependencies**: The ability to learn long-range dependencies is crucial in sequence\n",
       "modeling tasks. Self-attention reduces the maximum path length between any two input and output positions to a \n",
       "constant number of operations, making it easier to learn these dependencies. In recurrent networks, the path length\n",
       "grows linearly with the sequence length, which complicates the learning of relationships between distant elements \n",
       "in the sequence. Shorter paths in self-attention facilitate the learning of these long-range dependencies more \n",
       "effectively.\n",
       "\n",
       "In summary, self-attention's design allows for greater parallelization and shorter path lengths, which are critical\n",
       "for efficiently handling long-range dependencies in sequence data.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from rich import print\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "response = chat_model.invoke(f\"You are an expert in the field of AI and you are given a question and a document. You need to answer the question based on the document. Here is the question: Why does self-attention have an advantage over recurrent layers in terms of parallelization and path length for long-range dependencies? Here is the document: {result}\")\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
